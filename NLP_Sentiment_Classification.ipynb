{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WXaFSkUu0fzm"
   },
   "source": [
    "![alt text](https://drive.google.com/uc?export=view&id=1UXScsVx_Wni_JuDdB8LeTnM6jsPfIwkW)\n",
    "\n",
    "Proprietary content. Â© Great Learning. All Rights Reserved. Unauthorized use or distribution prohibited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OudB5by50jlI"
   },
   "source": [
    "# Sentiment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xT7MKZuMRaCg"
   },
   "source": [
    "### Dataset\n",
    "- Dataset of 50,000 movie reviews from IMDB, labeled by sentiment positive (1) or negative (0)\n",
    "- Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers).\n",
    "- For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\".\n",
    "- As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word.\n",
    "\n",
    "Command to import data\n",
    "- `from tensorflow.keras.datasets import imdb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q34-Y3nRKXdO"
   },
   "source": [
    "### Import the data (4 Marks)\n",
    "- Use `imdb.load_data()` method\n",
    "- Get train and test set\n",
    "- Take 10000 most frequent words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JxfwbrbuKbk2"
   },
   "source": [
    "import os\n",
    "from operator import itemgetter \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "from keras import models, regularizers, layers, optimizers, losses, metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import pandas as pd, numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "assert tf.__version__ >= '2.0'\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "# Keras\n",
    "from keras.layers import Dense, Embedding, LSTM, Dropout, MaxPooling1D, Conv1D\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from keras.datasets import imdb\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "random_state = 42\n",
    "np.random.seed(random_state)\n",
    "tf.random.set_seed(random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DldivBO4LTbP"
   },
   "source": [
    "### Pad each sentence to be of same length (4 Marks)\n",
    "- Take maximum sequence length as 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------ \n",
      "Number of rows in training dataset: 32000\n",
      "Number of columns in training dataset: 300\n",
      "Number of unique words in training dataset: 9999\n",
      "------------------------------------------------------------ \n",
      "Number of rows in validation dataset: 8000\n",
      "Number of columns in validation dataset: 300\n",
      "Number of unique words in validation dataset: 9984\n",
      "------------------------------------------------------------ \n",
      "Number of rows in test dataset: 10000\n",
      "Number of columns in test dataset: 300\n",
      "Number of unique words in test dataset: 9995\n",
      "------------------------------------------------------------ \n",
      "Unique Categories: (array([0, 1], dtype=int64), array([0, 1], dtype=int64), array([0, 1], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000\n",
    "maxlen = 300\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = vocab_size)\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen = maxlen, padding = 'pre')\n",
    "x_test =  pad_sequences(x_test, maxlen = maxlen, padding = 'pre')\n",
    "\n",
    "X = np.concatenate((x_train, x_test), axis = 0)\n",
    "y = np.concatenate((y_train, y_test), axis = 0)\n",
    "\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = random_state, shuffle = True)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size = 0.2, random_state = random_state, shuffle = True)\n",
    "\n",
    "print('---'*20, f'\\nNumber of rows in training dataset: {x_train.shape[0]}')\n",
    "print(f'Number of columns in training dataset: {x_train.shape[1]}')\n",
    "print(f'Number of unique words in training dataset: {len(np.unique(np.hstack(x_train)))}')\n",
    "\n",
    "\n",
    "print('---'*20, f'\\nNumber of rows in validation dataset: {x_valid.shape[0]}')\n",
    "print(f'Number of columns in validation dataset: {x_valid.shape[1]}')\n",
    "print(f'Number of unique words in validation dataset: {len(np.unique(np.hstack(x_valid)))}')\n",
    "\n",
    "\n",
    "print('---'*20, f'\\nNumber of rows in test dataset: {x_test.shape[0]}')\n",
    "print(f'Number of columns in test dataset: {x_test.shape[1]}')\n",
    "print(f'Number of unique words in test dataset: {len(np.unique(np.hstack(x_test)))}')\n",
    "\n",
    "\n",
    "print('---'*20, f'\\nUnique Categories: {np.unique(y_train), np.unique(y_valid), np.unique(y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "300\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,3):\n",
    "    print (len(x_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,    2,   18,  113],\n",
       "       [   0,    0,    0, ...,   39, 1889,    2],\n",
       "       [   0,    0,    0, ...,  433,   10,   10],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,   46,    7,   72],\n",
       "       [ 374,  500,    2, ..., 3492, 4294,  830],\n",
       "       [3063,   14, 1187, ...,   64,    6,  506]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JBFFCrybMSXz"
   },
   "source": [
    "### Print shape of features & labels (4 Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hdMCUPr7RaCm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32000, 300)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 300)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>33</td>\n",
       "      <td>222</td>\n",
       "      <td>13</td>\n",
       "      <td>124</td>\n",
       "      <td>13</td>\n",
       "      <td>28</td>\n",
       "      <td>77</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3088</td>\n",
       "      <td>11</td>\n",
       "      <td>63</td>\n",
       "      <td>36</td>\n",
       "      <td>2</td>\n",
       "      <td>68</td>\n",
       "      <td>555</td>\n",
       "      <td>39</td>\n",
       "      <td>1889</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>46</td>\n",
       "      <td>1501</td>\n",
       "      <td>5</td>\n",
       "      <td>271</td>\n",
       "      <td>39</td>\n",
       "      <td>78</td>\n",
       "      <td>8</td>\n",
       "      <td>433</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>244</td>\n",
       "      <td>1018</td>\n",
       "      <td>252</td>\n",
       "      <td>15</td>\n",
       "      <td>3633</td>\n",
       "      <td>36</td>\n",
       "      <td>80</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4752</td>\n",
       "      <td>908</td>\n",
       "      <td>310</td>\n",
       "      <td>8</td>\n",
       "      <td>202</td>\n",
       "      <td>178</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>786</td>\n",
       "      <td>2492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9    ...   290   291  292  \\\n",
       "0    0    0    0    0    0    0    0    0    0    0  ...    33   222   13   \n",
       "1    0    0    0    0    0    0    0    0    0    0  ...  3088    11   63   \n",
       "2    0    0    0    0    0    0    0    0    0    0  ...    46  1501    5   \n",
       "3    0    0    0    0    0    0    0    0    0    0  ...   244  1018  252   \n",
       "4    0    0    0    0    0    0    0    0    0    0  ...  4752   908  310   \n",
       "\n",
       "   293   294  295  296  297   298   299  \n",
       "0  124    13   28   77    2    18   113  \n",
       "1   36     2   68  555   39  1889     2  \n",
       "2  271    39   78    8  433    10    10  \n",
       "3   15  3633   36   80    2    15    99  \n",
       "4    8   202  178    6    2   786  2492  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.DataFrame.from_records(x_train)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of review, number of words in each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eGVHeKOWyJiG"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>32000.000000</td>\n",
       "      <td>32000.000000</td>\n",
       "      <td>32000.000000</td>\n",
       "      <td>32000.000000</td>\n",
       "      <td>32000.000000</td>\n",
       "      <td>32000.000000</td>\n",
       "      <td>32000.000000</td>\n",
       "      <td>32000.000000</td>\n",
       "      <td>32000.000000</td>\n",
       "      <td>32000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>32000.000000</td>\n",
       "      <td>32000.000000</td>\n",
       "      <td>32000.000000</td>\n",
       "      <td>32000.000000</td>\n",
       "      <td>32000.000000</td>\n",
       "      <td>32000.000000</td>\n",
       "      <td>32000.000000</td>\n",
       "      <td>32000.000000</td>\n",
       "      <td>32000.000000</td>\n",
       "      <td>32000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>172.157281</td>\n",
       "      <td>175.706562</td>\n",
       "      <td>179.174625</td>\n",
       "      <td>181.260844</td>\n",
       "      <td>180.998781</td>\n",
       "      <td>179.397469</td>\n",
       "      <td>180.618250</td>\n",
       "      <td>183.153781</td>\n",
       "      <td>181.302312</td>\n",
       "      <td>191.734531</td>\n",
       "      <td>...</td>\n",
       "      <td>644.957750</td>\n",
       "      <td>633.282250</td>\n",
       "      <td>622.849625</td>\n",
       "      <td>636.573281</td>\n",
       "      <td>633.476344</td>\n",
       "      <td>634.363781</td>\n",
       "      <td>631.616844</td>\n",
       "      <td>576.730531</td>\n",
       "      <td>645.384688</td>\n",
       "      <td>1046.984875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>851.409513</td>\n",
       "      <td>861.322390</td>\n",
       "      <td>884.413498</td>\n",
       "      <td>864.840651</td>\n",
       "      <td>890.567717</td>\n",
       "      <td>868.222766</td>\n",
       "      <td>879.500435</td>\n",
       "      <td>887.351445</td>\n",
       "      <td>869.003919</td>\n",
       "      <td>904.680674</td>\n",
       "      <td>...</td>\n",
       "      <td>1504.123808</td>\n",
       "      <td>1477.307788</td>\n",
       "      <td>1450.853459</td>\n",
       "      <td>1473.621678</td>\n",
       "      <td>1473.942470</td>\n",
       "      <td>1468.729612</td>\n",
       "      <td>1461.547991</td>\n",
       "      <td>1373.699426</td>\n",
       "      <td>1464.213046</td>\n",
       "      <td>1819.475102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>46.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>250.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>422.000000</td>\n",
       "      <td>410.000000</td>\n",
       "      <td>412.000000</td>\n",
       "      <td>432.000000</td>\n",
       "      <td>422.250000</td>\n",
       "      <td>435.250000</td>\n",
       "      <td>437.000000</td>\n",
       "      <td>387.000000</td>\n",
       "      <td>470.000000</td>\n",
       "      <td>1109.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>9941.000000</td>\n",
       "      <td>9995.000000</td>\n",
       "      <td>9992.000000</td>\n",
       "      <td>9945.000000</td>\n",
       "      <td>9996.000000</td>\n",
       "      <td>9980.000000</td>\n",
       "      <td>9996.000000</td>\n",
       "      <td>9990.000000</td>\n",
       "      <td>9901.000000</td>\n",
       "      <td>9989.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>9988.000000</td>\n",
       "      <td>9996.000000</td>\n",
       "      <td>9994.000000</td>\n",
       "      <td>9986.000000</td>\n",
       "      <td>9996.000000</td>\n",
       "      <td>9998.000000</td>\n",
       "      <td>9997.000000</td>\n",
       "      <td>9997.000000</td>\n",
       "      <td>9984.000000</td>\n",
       "      <td>9990.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0             1             2             3             4    \\\n",
       "count  32000.000000  32000.000000  32000.000000  32000.000000  32000.000000   \n",
       "mean     172.157281    175.706562    179.174625    181.260844    180.998781   \n",
       "std      851.409513    861.322390    884.413498    864.840651    890.567717   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max     9941.000000   9995.000000   9992.000000   9945.000000   9996.000000   \n",
       "\n",
       "                5             6             7             8             9    \\\n",
       "count  32000.000000  32000.000000  32000.000000  32000.000000  32000.000000   \n",
       "mean     179.397469    180.618250    183.153781    181.302312    191.734531   \n",
       "std      868.222766    879.500435    887.351445    869.003919    904.680674   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max     9980.000000   9996.000000   9990.000000   9901.000000   9989.000000   \n",
       "\n",
       "       ...           290           291           292           293  \\\n",
       "count  ...  32000.000000  32000.000000  32000.000000  32000.000000   \n",
       "mean   ...    644.957750    633.282250    622.849625    636.573281   \n",
       "std    ...   1504.123808   1477.307788   1450.853459   1473.621678   \n",
       "min    ...      0.000000      0.000000      0.000000      1.000000   \n",
       "25%    ...     12.000000     12.000000     12.000000     12.000000   \n",
       "50%    ...     62.000000     61.000000     62.000000     62.000000   \n",
       "75%    ...    422.000000    410.000000    412.000000    432.000000   \n",
       "max    ...   9988.000000   9996.000000   9994.000000   9986.000000   \n",
       "\n",
       "                294           295           296           297           298  \\\n",
       "count  32000.000000  32000.000000  32000.000000  32000.000000  32000.000000   \n",
       "mean     633.476344    634.363781    631.616844    576.730531    645.384688   \n",
       "std     1473.942470   1468.729612   1461.547991   1373.699426   1464.213046   \n",
       "min        2.000000      2.000000      2.000000      2.000000      2.000000   \n",
       "25%       12.000000     12.000000     12.000000     11.000000     10.000000   \n",
       "50%       65.000000     65.000000     65.000000     54.000000     68.000000   \n",
       "75%      422.250000    435.250000    437.000000    387.000000    470.000000   \n",
       "max     9996.000000   9998.000000   9997.000000   9997.000000   9984.000000   \n",
       "\n",
       "                299  \n",
       "count  32000.000000  \n",
       "mean    1046.984875  \n",
       "std     1819.475102  \n",
       "min        2.000000  \n",
       "25%       46.000000  \n",
       "50%      250.000000  \n",
       "75%     1109.000000  \n",
       "max     9990.000000  \n",
       "\n",
       "[8 rows x 300 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: \n",
      "(50000, 300)\n",
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "# summarize size\n",
    "print(\"Training data: \")\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: \n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "# Summarize number of classes\n",
    "print(\"Classes: \")\n",
    "print(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: \n",
      "9999\n"
     ]
    }
   ],
   "source": [
    "# Summarize number of words\n",
    "print(\"Number of words: \")\n",
    "print(len(np.unique(np.hstack(X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review length: \n",
      "Mean 300.00 words (0.000000)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Summarize review length\n",
    "print(\"Review length: \")\n",
    "result = [len(x) for x in X]\n",
    "print(\"Mean %.2f words (%f)\" % (np.mean(result), np.std(result)))\n",
    "# plot review length\n",
    "plt.boxplot(result)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5cNk5sDvMr3j"
   },
   "source": [
    "Number of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n",
    "num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Z00-mYgMoKv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_labels  (25000,)\n",
      "test_labels  (25000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"train_labels \", train_labels.shape)\n",
    "print(\"test_labels \", test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NdXPWuOmNEbh"
   },
   "source": [
    "### Print value of any one feature and it's label (4 Marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MGLEdeFmNZfR"
   },
   "source": [
    "Feature value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h_85Hqm0Nb1I"
   },
   "source": [
    "Label value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-FoehB5jNd1g"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0cof4LSxNxuv"
   },
   "source": [
    "### Decode the feature value to get original sentence (4 Marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q_oiAyPZOkJD"
   },
   "source": [
    "First, retrieve a dictionary that contains mapping of words to their index in the IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Clsk-yK8OtzD"
   },
   "outputs": [],
   "source": [
    "def decode_review(x, y):\n",
    "  w2i = imdb.get_word_index()                                \n",
    "  w2i = {k:(v + 3) for k, v in w2i.items()}\n",
    "  w2i['<PAD>'] = 0\n",
    "  w2i['<START>'] = 1\n",
    "  w2i['<UNK>'] = 2\n",
    "  i2w = {i: w for w, i in w2i.items()}\n",
    "\n",
    "  ws = (' '.join(i2w[i] for i in x))\n",
    "  print(f'Review: {ws}')\n",
    "  print(f'Actual Sentiment: {y}')\n",
    "  return w2i, i2w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NRgOD5S2Uuvd"
   },
   "source": [
    "Now use the dictionary to get the original words from the encodings, for a particular sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zJ504QDORwxj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <START> i watch this movie without big expectations i think everyone should do it's a great tv <UNK> and of course we couldn't compare it with gone with the wind but it's still nice to watch it's also weird to see a different scarlett <UNK> <UNK> don't play scarlett with passion and fire like <UNK> leigh but i believe that scarlett is changed when she became older don't expect to much of this just watch but don't watch like i think this would be horrible\n",
      "Actual Sentiment: 1\n",
      "------------------------------------------------------------------------------------------ \n",
      " [(34704, 'fawn'), (52009, 'tsukino'), (52010, 'nunnery'), (16819, 'sonja'), (63954, 'vani'), (1411, 'woods'), (16118, 'spiders'), (2348, 'hanging'), (2292, 'woody'), (52011, 'trawling'), (52012, \"hold's\"), (11310, 'comically'), (40833, 'localized'), (30571, 'disobeying'), (52013, \"'royale\"), (40834, \"harpo's\"), (52014, 'canet'), (19316, 'aileen'), (52015, 'acurately'), (52016, \"diplomat's\"), (25245, 'rickman'), (6749, 'arranged'), (52017, 'rumbustious'), (52018, 'familiarness'), (52019, \"spider'\"), (68807, 'hahahah'), (52020, \"wood'\"), (40836, 'transvestism'), (34705, \"hangin'\"), (2341, 'bringing'), (40837, 'seamier'), (34706, 'wooded'), (52021, 'bravora'), (16820, 'grueling'), (1639, 'wooden'), (16821, 'wednesday'), (52022, \"'prix\"), (34707, 'altagracia'), (52023, 'circuitry'), (11588, 'crotch'), (57769, 'busybody'), (52024, \"tart'n'tangy\"), (14132, 'burgade'), (52026, 'thrace'), (11041, \"tom's\"), (52028, 'snuggles'), (29117, 'francesco'), (52030, 'complainers'), (52128, 'templarios'), (40838, '272')]\n"
     ]
    }
   ],
   "source": [
    "w2i, i2w = decode_review(x_train[200], y_train[200])\n",
    "# get first 50 key, value pairs from id to word dictionary\n",
    "print('---'*30, '\\n', list(islice(i2w.items(), 0, 50)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WLGABrJoVZe6"
   },
   "source": [
    "Get the sentiment for the above sentence\n",
    "- positive (1)\n",
    "- negative (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XDyQGJT0Ve-a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Sentiment: [1 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(f'Actual Sentiment: {y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BmCjr8miXIWB"
   },
   "source": [
    "### Define model (10 Marks)\n",
    "- Define a Sequential Model\n",
    "- Add Embedding layer\n",
    "  - Embedding layer turns positive integers into dense vectors of fixed size\n",
    "  - `tensorflow.keras` embedding layer doesn't require us to onehot encode our words, instead we have to give each word a unique integer number as an id. For the imdb dataset we've loaded this has already been done, but if this wasn't the case we could use sklearn LabelEncoder.\n",
    "  - Size of the vocabulary will be 10000\n",
    "  - Give dimension of the dense embedding as 100\n",
    "  - Length of input sequences should be 300\n",
    "- Add LSTM layer\n",
    "  - Pass value in `return_sequences` as True\n",
    "- Add a `TimeDistributed` layer with 100 Dense neurons\n",
    "- Add Flatten layer\n",
    "- Add Dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Np5GxT1caFEq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "#Define a sequential model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, Embedding, LSTM, Dropout\n",
    "print('Build model...')\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding layer\n",
    "voca_size = 10000\n",
    "dimen=100\n",
    "leng=300\n",
    "model.add(Embedding(voca_size, dimen,input_length=leng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 300, 100)\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM layer\n",
    "model.add(LSTM(64, dropout=0.2,return_sequences=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten layer\n",
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense layer\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hc4bknOobDby"
   },
   "source": [
    "### Compile the model (4 Marks)\n",
    "- Use Optimizer as Adam\n",
    "- Use Binary Crossentropy as loss\n",
    "- Use Accuracy as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jw4RJ0CQbwFY"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8sEzwazqbz3T"
   },
   "source": [
    "### Print model summary (4 Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Hx1yxwlb2Ue"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 300, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 300, 64)           42240     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 19200)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 19201     \n",
      "=================================================================\n",
      "Total params: 1,061,441\n",
      "Trainable params: 1,061,441\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding callbacks\n",
    "es = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 0)  \n",
    "mc = ModelCheckpoint('imdb_model.h5', monitor = 'val_loss', mode = 'min', save_best_only = True, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bmkolKP4b-U6"
   },
   "source": [
    "### Fit the model (4 Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32000, 300)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32000,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "500/500 [==============================] - 147s 295ms/step - loss: 0.3475 - accuracy: 0.8408 - val_loss: 0.2733 - val_accuracy: 0.8846\n",
      "Epoch 2/3\n",
      "500/500 [==============================] - 145s 289ms/step - loss: 0.1997 - accuracy: 0.9214 - val_loss: 0.2595 - val_accuracy: 0.8944\n",
      "Epoch 3/3\n",
      "500/500 [==============================] - 150s 301ms/step - loss: 0.1302 - accuracy: 0.9519 - val_loss: 0.2902 - val_accuracy: 0.8880\n"
     ]
    }
   ],
   "source": [
    "NumEpochs = 3\n",
    "BatchSize = 64\n",
    "history = model.fit(x_train, y_train, validation_data = (x_valid, y_valid), epochs = 3, batch_size = 64, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bwLl54MXnkEA"
   },
   "source": [
    "### Evaluate model (4 Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 13s 82ms/step - loss: 0.2863 - accuracy: 0.8920\n",
      "Test accuracy: 89.20%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "scores = model.evaluate(x_test, y_test, batch_size = 64)\n",
    "print('Test accuracy: %.2f%%' % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h2amr1tJn9Jz"
   },
   "source": [
    "### Predict on one sample (4 Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-34-8ec3f06dc8c9>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89      5022\n",
      "           1       0.88      0.90      0.89      4978\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.89      0.89      0.89     10000\n",
      "weighted avg       0.89      0.89      0.89     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict_classes(x_test)\n",
    "print(f'Classification Report:\\n{classification_report(y_pred, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <START> this movie was great and i was waiting for it for a long time when it finally came out i was really happy and looked forward to a 10 out of 10 it was great and lived up to my potential the performances were great on the part of the adults and most of the kids the only bad performance was by milo himself there was one problem that i encountered with this and others like it movie all of the characters i wanted to live were getting killed overall i give this movie an excellent 9 out of 10 maybe we should <UNK> better people to kill next time though ok\n",
      "Actual Sentiment: 1\n",
      "Predicted sentiment: 1\n"
     ]
    }
   ],
   "source": [
    "decode_review(x_test[10], y_test[10])\n",
    "print(f'Predicted sentiment: {y_pred[10][0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "### Sentiment classification task on the IMDB dataset, on test dataset,\n",
    "#### * Accuracy: > 88%\n",
    "#### * F1-score: = 89%\n",
    "#### * Loss of 0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pdbXlqq17W6a"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Questions - Project 1 - Sequential Models in NLP - Sentiment Classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
